{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CVFEr0r9M2MS",
    "outputId": "c3ced908-dd7e-42ff-b5c6-4e5da2984d4e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D \n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.layers.core import Activation, Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Input, Permute,  Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers import merge, Multiply\n",
    "\n",
    "K.set_image_dim_ordering(\"th\")\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "\n",
    "\n",
    "def attention_3d_block(inputs,timesteps):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    time_steps=timesteps\n",
    "    print(input_dim)\n",
    "    print(time_steps)\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, time_steps))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a) #, name='dim_reduction'\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1))(a) #, name='attention_vec ' % (depth)\n",
    "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul= Multiply()([inputs, a_probs]) #name='attention_mul'\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "class LSTMATTBFnet:\n",
    "    @staticmethod\n",
    "    def build(timeSteps,variables,classes):\n",
    "        inputNet = Input(shape=(timeSteps,variables)) #batch_shape=(20, 7, 5) \n",
    "        #lstm=Bidirectional(LSTM(100,recurrent_dropout=0.4,dropout=0.4),merge_mode='concat')(inputNet) #worse using stateful=True\n",
    "\n",
    "        lstm=LSTM(100,recurrent_dropout=0.1,dropout=0.1,return_sequences=True)(inputNet) #worse using stateful=True \n",
    "        attention_mul = attention_3d_block(lstm,timeSteps)\n",
    "        attention_mul = Flatten()(attention_mul)\n",
    "        # a softmax classifier\n",
    "        classificationLayer=Dense(classes,activation='softmax')(attention_mul)\n",
    "        \n",
    "        model=Model(inputNet,classificationLayer)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "bV85vySlM2MV",
    "outputId": "b23f9497-d004-4f82-df1f-af6d07f75429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# total shapes #############\n",
      "(2876, 72, 8)\n",
      "2876\n",
      "############# train shapes #############\n",
      "(2301, 72, 8)\n",
      "(2301, 1)\n",
      "############# test shapes #############\n",
      "(575, 72, 8)\n",
      "(575, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjpg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def non_shuffling_train_test_split(X, y, test_size=0.2):\n",
    "    i = int((1 - test_size) * X.shape[0]) + 1\n",
    "    X_train, X_test = np.split(X, [i])\n",
    "    y_train, y_test = np.split(y, [i])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# load all data\n",
    "dfin = pd.read_csv('windows-size72-step12.csv',header=None)  #inputs 72 lines per sample\n",
    "dfout = pd.read_csv('out-class-size72-step12.csv',header=None)  #output classes \n",
    "\n",
    "total_inputs,total_output = dfin.as_matrix().astype(np.float32),dfout.as_matrix().astype(np.int32)\n",
    "\n",
    "# normalize\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "total_inputs = scaler.fit_transform(total_inputs)\n",
    "\n",
    "# every 72 lines is one input sample\n",
    "total_inputs = np.reshape(total_inputs, (-1,72,8))\n",
    "print(\"############# total shapes #############\")\n",
    "print(total_inputs.shape)\n",
    "print(total_output.size)\n",
    "\n",
    "train_inputs, test_inputs, train_output, test_output = non_shuffling_train_test_split(total_inputs, total_output, test_size=0.2)\n",
    "print(\"############# train shapes #############\")\n",
    "print(train_inputs.shape)\n",
    "print(train_output.shape)\n",
    "\n",
    "print(\"############# test shapes #############\")\n",
    "print(test_inputs.shape)\n",
    "print(test_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "colab_type": "code",
    "id": "QH6FD43fM2Mc",
    "outputId": "303da2f8-16e7-40ad-9780-8eabf27d4ab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(575, 5)\n",
      "100\n",
      "72\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 100 and 72 for 'dense_1_1/add' (op: 'Add') with input shapes: [?,100,72], [1,72,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1588\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 100 and 72 for 'dense_1_1/add' (op: 'Add') with input shapes: [?,100,72], [1,72,1].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fab0020ee9a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mOPTIMIZER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMATTBFnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeSteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_ROWS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_COLS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNB_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n\u001b[1;32m     15\u001b[0m \tmetrics=[\"accuracy\"])\n",
      "\u001b[0;32m<ipython-input-5-bc12464dd45f>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(timeSteps, variables, classes)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mlstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputNet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#worse using stateful=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mattention_mul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_3d_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeSteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mattention_mul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mul\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# a softmax classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-bc12464dd45f>\u001b[0m in \u001b[0;36mattention_3d_block\u001b[0;34m(inputs, timesteps)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#a = Reshape((input_dim, time_steps))(a) # this line is not useful. It's just to know which dimension is what.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mSINGLE_ATTENTION_VECTOR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, name='dim_reduction'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(x, bias, data_format)\u001b[0m\n\u001b[1;32m   4018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 297\u001b[0;31m         \"Add\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3412\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3413\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3414\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1754\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1755\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1756\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1757\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 100 and 72 for 'dense_1_1/add' (op: 'Add') with input shapes: [?,100,72], [1,72,1]."
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_ROWS, IMG_COLS = 72, 8 # input image dimensions\n",
    "NB_CLASSES = 5  # number of outputs = number of classes\n",
    "\n",
    "X_train = train_inputs\n",
    "y_train = np_utils.to_categorical(train_output, NB_CLASSES)\n",
    "X_test = test_inputs\n",
    "y_test = np_utils.to_categorical(test_output, NB_CLASSES)\n",
    "print(y_test.shape)\n",
    "\n",
    "OPTIMIZER = Adam()\n",
    "\n",
    "model = LSTMATTBFnet.build(timeSteps=IMG_ROWS,variables=IMG_COLS,classes=NB_CLASSES)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "colab_type": "code",
    "id": "1NNjAwLJNtMC",
    "outputId": "9878b8fb-8bc5-4590-a9d9-d0513ce128ce"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"636pt\" viewBox=\"0.00 0.00 455.00 636.00\" width=\"455pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 632)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-632 451,-632 451,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140321453564312 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140321453564312</title>\n",
       "<polygon fill=\"none\" points=\"96.5,-581.5 96.5,-627.5 382.5,-627.5 382.5,-581.5 96.5,-581.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-600.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"229.5,-581.5 229.5,-627.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"229.5,-604.5 287.5,-604.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"287.5,-581.5 287.5,-627.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335\" y=\"-612.3\">(None, 72, 8)</text>\n",
       "<polyline fill=\"none\" points=\"287.5,-604.5 382.5,-604.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"335\" y=\"-589.3\">(None, 72, 8)</text>\n",
       "</g>\n",
       "<!-- 140321453565208 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140321453565208</title>\n",
       "<polygon fill=\"none\" points=\"104.5,-498.5 104.5,-544.5 374.5,-544.5 374.5,-498.5 104.5,-498.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-517.8\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"206.5,-498.5 206.5,-544.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"206.5,-521.5 264.5,-521.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"264.5,-498.5 264.5,-544.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319.5\" y=\"-529.3\">(None, 72, 8)</text>\n",
       "<polyline fill=\"none\" points=\"264.5,-521.5 374.5,-521.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319.5\" y=\"-506.3\">(None, 72, 100)</text>\n",
       "</g>\n",
       "<!-- 140321453564312&#45;&gt;140321453565208 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140321453564312-&gt;140321453565208</title>\n",
       "<path d=\"M239.5,-581.3799C239.5,-573.1745 239.5,-563.7679 239.5,-554.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"243.0001,-554.784 239.5,-544.784 236.0001,-554.784 243.0001,-554.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140321400564648 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140321400564648</title>\n",
       "<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 301,-461.5 301,-415.5 0,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66.5\" y=\"-434.8\">permute_1: Permute</text>\n",
       "<polyline fill=\"none\" points=\"133,-415.5 133,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"133,-438.5 191,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"191,-415.5 191,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246\" y=\"-446.3\">(None, 72, 100)</text>\n",
       "<polyline fill=\"none\" points=\"191,-438.5 301,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246\" y=\"-423.3\">(None, 100, 72)</text>\n",
       "</g>\n",
       "<!-- 140321453565208&#45;&gt;140321400564648 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140321453565208-&gt;140321400564648</title>\n",
       "<path d=\"M214.7085,-498.3799C204.8581,-489.1935 193.393,-478.5013 182.8994,-468.7152\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"185.1676,-466.0446 175.4672,-461.784 180.3934,-471.1639 185.1676,-466.0446\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140321400565432 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140321400565432</title>\n",
       "<polygon fill=\"none\" points=\"32,-166.5 32,-212.5 447,-212.5 447,-166.5 32,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"101\" y=\"-185.8\">multiply_1: Multiply</text>\n",
       "<polyline fill=\"none\" points=\"170,-166.5 170,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"199\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"170,-189.5 228,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"199\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"228,-166.5 228,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"337.5\" y=\"-197.3\">[(None, 72, 100), (None, 72, 100)]</text>\n",
       "<polyline fill=\"none\" points=\"228,-189.5 447,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"337.5\" y=\"-174.3\">(None, 72, 100)</text>\n",
       "</g>\n",
       "<!-- 140321453565208&#45;&gt;140321400565432 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140321453565208-&gt;140321400565432</title>\n",
       "<path d=\"M278.3433,-498.2713C290.6597,-488.7226 302.8053,-476.4768 309.5,-462 349.2347,-376.076 349.2347,-334.924 309.5,-249 304.2175,-237.5769 295.5411,-227.5429 286.0649,-219.1269\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"288.2765,-216.414 278.3433,-212.7287 283.8102,-221.8041 288.2765,-216.414\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140321399080552 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140321399080552</title>\n",
       "<polygon fill=\"none\" points=\"13,-332.5 13,-378.5 288,-378.5 288,-332.5 13,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66.5\" y=\"-351.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"120,-332.5 120,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"120,-355.5 178,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"178,-332.5 178,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-363.3\">(None, 100, 72)</text>\n",
       "<polyline fill=\"none\" points=\"178,-355.5 288,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-340.3\">(None, 100, 72)</text>\n",
       "</g>\n",
       "<!-- 140321400564648&#45;&gt;140321399080552 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140321400564648-&gt;140321399080552</title>\n",
       "<path d=\"M150.5,-415.3799C150.5,-407.1745 150.5,-397.7679 150.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"154.0001,-388.784 150.5,-378.784 147.0001,-388.784 154.0001,-388.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140321400564144 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140321400564144</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 301,-295.5 301,-249.5 0,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66.5\" y=\"-268.8\">permute_2: Permute</text>\n",
       "<polyline fill=\"none\" points=\"133,-249.5 133,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"133,-272.5 191,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"191,-249.5 191,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246\" y=\"-280.3\">(None, 100, 72)</text>\n",
       "<polyline fill=\"none\" points=\"191,-272.5 301,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246\" y=\"-257.3\">(None, 72, 100)</text>\n",
       "</g>\n",
       "<!-- 140321399080552&#45;&gt;140321400564144 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140321399080552-&gt;140321400564144</title>\n",
       "<path d=\"M150.5,-332.3799C150.5,-324.1745 150.5,-314.7679 150.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"154.0001,-305.784 150.5,-295.784 147.0001,-305.784 154.0001,-305.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140321400564144&#45;&gt;140321400565432 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140321400564144-&gt;140321400565432</title>\n",
       "<path d=\"M175.2915,-249.3799C185.1419,-240.1935 196.607,-229.5013 207.1006,-219.7152\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"209.6066,-222.1639 214.5328,-212.784 204.8324,-217.0446 209.6066,-222.1639\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140321398769312 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140321398769312</title>\n",
       "<polygon fill=\"none\" points=\"99,-83.5 99,-129.5 380,-129.5 380,-83.5 99,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-102.8\">flatten_1: Flatten</text>\n",
       "<polyline fill=\"none\" points=\"212,-83.5 212,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"212,-106.5 270,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"270,-83.5 270,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325\" y=\"-114.3\">(None, 72, 100)</text>\n",
       "<polyline fill=\"none\" points=\"270,-106.5 380,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325\" y=\"-91.3\">(None, 7200)</text>\n",
       "</g>\n",
       "<!-- 140321400565432&#45;&gt;140321398769312 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140321400565432-&gt;140321398769312</title>\n",
       "<path d=\"M239.5,-166.3799C239.5,-158.1745 239.5,-148.7679 239.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"243.0001,-139.784 239.5,-129.784 236.0001,-139.784 243.0001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140321398977480 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140321398977480</title>\n",
       "<polygon fill=\"none\" points=\"109.5,-.5 109.5,-46.5 369.5,-46.5 369.5,-.5 109.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-19.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"216.5,-.5 216.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"245.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"216.5,-23.5 274.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"245.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"274.5,-.5 274.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322\" y=\"-31.3\">(None, 7200)</text>\n",
       "<polyline fill=\"none\" points=\"274.5,-23.5 369.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322\" y=\"-8.3\">(None, 5)</text>\n",
       "</g>\n",
       "<!-- 140321398769312&#45;&gt;140321398977480 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140321398769312-&gt;140321398977480</title>\n",
       "<path d=\"M239.5,-83.3799C239.5,-75.1745 239.5,-65.7679 239.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"243.0001,-56.784 239.5,-46.784 236.0001,-56.784 243.0001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import SVG,display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "modelSVG=SVG(model_to_dot(model,show_shapes=True).create(prog='dot', format='svg'))\n",
    "display(modelSVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7309
    },
    "colab_type": "code",
    "id": "zpWno9_7PNh-",
    "outputId": "89ab8e16-d51f-462e-e523-80fcbf6c5db4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0615 14:11:30.380663 140322434881408 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2301 samples, validate on 575 samples\n",
      "Epoch 1/100\n",
      "2301/2301 [==============================] - 7s 3ms/step - loss: 1.5886 - acc: 0.2542 - val_loss: 1.6377 - val_acc: 0.2296\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.22957, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 2/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.5634 - acc: 0.3007 - val_loss: 1.5863 - val_acc: 0.2174\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.22957\n",
      "Epoch 3/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.5747 - acc: 0.2625 - val_loss: 1.5755 - val_acc: 0.2313\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.22957 to 0.23130, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 4/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.5249 - acc: 0.3060 - val_loss: 1.5361 - val_acc: 0.2417\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.23130 to 0.24174, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 5/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.5116 - acc: 0.3168 - val_loss: 1.5443 - val_acc: 0.2139\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.24174\n",
      "Epoch 6/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.5081 - acc: 0.3190 - val_loss: 1.5311 - val_acc: 0.2574\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.24174 to 0.25739, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 7/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4885 - acc: 0.3294 - val_loss: 1.5083 - val_acc: 0.2557\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.25739\n",
      "Epoch 8/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4837 - acc: 0.3346 - val_loss: 1.4990 - val_acc: 0.2643\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.25739 to 0.26435, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 9/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4808 - acc: 0.3368 - val_loss: 1.5003 - val_acc: 0.2522\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.26435\n",
      "Epoch 10/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4699 - acc: 0.3512 - val_loss: 1.4961 - val_acc: 0.2852\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.26435 to 0.28522, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 11/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4674 - acc: 0.3577 - val_loss: 1.4940 - val_acc: 0.2904\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.28522 to 0.29043, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 12/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4666 - acc: 0.3451 - val_loss: 1.4729 - val_acc: 0.2974\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.29043 to 0.29739, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 13/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4841 - acc: 0.3381 - val_loss: 1.4927 - val_acc: 0.2574\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.29739\n",
      "Epoch 14/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4603 - acc: 0.3551 - val_loss: 1.4871 - val_acc: 0.2591\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.29739\n",
      "Epoch 15/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4473 - acc: 0.3542 - val_loss: 1.4987 - val_acc: 0.2609\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.29739\n",
      "Epoch 16/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4492 - acc: 0.3638 - val_loss: 1.4791 - val_acc: 0.2643\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.29739\n",
      "Epoch 17/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4501 - acc: 0.3607 - val_loss: 1.4669 - val_acc: 0.2904\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.29739\n",
      "Epoch 18/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4500 - acc: 0.3546 - val_loss: 1.4681 - val_acc: 0.3409\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.29739 to 0.34087, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 19/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4511 - acc: 0.3620 - val_loss: 1.4512 - val_acc: 0.3183\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.34087\n",
      "Epoch 20/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4329 - acc: 0.3755 - val_loss: 1.4755 - val_acc: 0.2817\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.34087\n",
      "Epoch 21/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4399 - acc: 0.3638 - val_loss: 1.4600 - val_acc: 0.3113\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.34087\n",
      "Epoch 22/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4497 - acc: 0.3559 - val_loss: 1.4635 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.34087 to 0.37043, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 23/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4311 - acc: 0.3707 - val_loss: 1.4462 - val_acc: 0.2922\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.37043\n",
      "Epoch 24/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4228 - acc: 0.3651 - val_loss: 1.4406 - val_acc: 0.3304\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.37043\n",
      "Epoch 25/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4077 - acc: 0.3768 - val_loss: 1.4228 - val_acc: 0.3391\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.37043\n",
      "Epoch 26/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3945 - acc: 0.3811 - val_loss: 1.4200 - val_acc: 0.3600\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.37043\n",
      "Epoch 27/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.4118 - acc: 0.3820 - val_loss: 1.4212 - val_acc: 0.3409\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.37043\n",
      "Epoch 28/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3950 - acc: 0.3790 - val_loss: 1.4168 - val_acc: 0.3600\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.37043\n",
      "Epoch 29/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3953 - acc: 0.3716 - val_loss: 1.4013 - val_acc: 0.3304\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.37043\n",
      "Epoch 30/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3864 - acc: 0.3903 - val_loss: 1.4201 - val_acc: 0.3843\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.37043 to 0.38435, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 31/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3864 - acc: 0.3972 - val_loss: 1.3941 - val_acc: 0.3565\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.38435\n",
      "Epoch 32/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3717 - acc: 0.3933 - val_loss: 1.4159 - val_acc: 0.3687\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.38435\n",
      "Epoch 33/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3660 - acc: 0.3998 - val_loss: 1.4012 - val_acc: 0.3739\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.38435\n",
      "Epoch 34/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3617 - acc: 0.3972 - val_loss: 1.3935 - val_acc: 0.3757\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.38435\n",
      "Epoch 35/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3679 - acc: 0.3963 - val_loss: 1.3996 - val_acc: 0.3687\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.38435\n",
      "Epoch 36/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3510 - acc: 0.4181 - val_loss: 1.3847 - val_acc: 0.3687\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.38435\n",
      "Epoch 37/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3473 - acc: 0.4050 - val_loss: 1.3903 - val_acc: 0.3809\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.38435\n",
      "Epoch 38/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3506 - acc: 0.4033 - val_loss: 1.3861 - val_acc: 0.3565\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.38435\n",
      "Epoch 39/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3410 - acc: 0.4103 - val_loss: 1.3781 - val_acc: 0.3913\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.38435 to 0.39130, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 40/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3280 - acc: 0.4168 - val_loss: 1.3806 - val_acc: 0.4070\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.39130 to 0.40696, saving model to LSTM-ATTENTION-BF-energie.h5\n",
      "Epoch 41/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3310 - acc: 0.4103 - val_loss: 1.3656 - val_acc: 0.3791\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.40696\n",
      "Epoch 42/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3241 - acc: 0.4094 - val_loss: 1.3806 - val_acc: 0.3878\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.40696\n",
      "Epoch 43/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3076 - acc: 0.4298 - val_loss: 1.3693 - val_acc: 0.3826\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.40696\n",
      "Epoch 44/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.3103 - acc: 0.4333 - val_loss: 1.3795 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.40696\n",
      "Epoch 45/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2967 - acc: 0.4337 - val_loss: 1.3703 - val_acc: 0.3983\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.40696\n",
      "Epoch 46/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2890 - acc: 0.4307 - val_loss: 1.3563 - val_acc: 0.3965\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.40696\n",
      "Epoch 47/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2920 - acc: 0.4455 - val_loss: 1.3633 - val_acc: 0.3896\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.40696\n",
      "Epoch 48/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2748 - acc: 0.4424 - val_loss: 1.3670 - val_acc: 0.3757\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.40696\n",
      "Epoch 49/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2798 - acc: 0.4402 - val_loss: 1.3747 - val_acc: 0.3774\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.40696\n",
      "Epoch 50/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2627 - acc: 0.4563 - val_loss: 1.3751 - val_acc: 0.3930\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.40696\n",
      "Epoch 51/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2628 - acc: 0.4563 - val_loss: 1.3619 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.40696\n",
      "Epoch 52/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2484 - acc: 0.4750 - val_loss: 1.3633 - val_acc: 0.3826\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.40696\n",
      "Epoch 53/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2398 - acc: 0.4668 - val_loss: 1.3680 - val_acc: 0.3861\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.40696\n",
      "Epoch 54/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2395 - acc: 0.4720 - val_loss: 1.3701 - val_acc: 0.3878\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.40696\n",
      "Epoch 55/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2247 - acc: 0.4772 - val_loss: 1.3709 - val_acc: 0.3878\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.40696\n",
      "Epoch 56/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2245 - acc: 0.4624 - val_loss: 1.3705 - val_acc: 0.3861\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.40696\n",
      "Epoch 57/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2338 - acc: 0.4694 - val_loss: 1.3742 - val_acc: 0.3843\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.40696\n",
      "Epoch 58/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2335 - acc: 0.4624 - val_loss: 1.3774 - val_acc: 0.3965\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.40696\n",
      "Epoch 59/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2270 - acc: 0.4841 - val_loss: 1.3733 - val_acc: 0.3878\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.40696\n",
      "Epoch 60/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2220 - acc: 0.4724 - val_loss: 1.3748 - val_acc: 0.3809\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.40696\n",
      "Epoch 61/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2260 - acc: 0.4811 - val_loss: 1.3748 - val_acc: 0.3843\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.40696\n",
      "Epoch 62/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2213 - acc: 0.4833 - val_loss: 1.3760 - val_acc: 0.3843\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.40696\n",
      "Epoch 63/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2222 - acc: 0.4867 - val_loss: 1.3754 - val_acc: 0.3913\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.40696\n",
      "Epoch 64/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2277 - acc: 0.4785 - val_loss: 1.3734 - val_acc: 0.3930\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.40696\n",
      "Epoch 65/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2148 - acc: 0.4785 - val_loss: 1.3704 - val_acc: 0.3930\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.40696\n",
      "Epoch 66/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2179 - acc: 0.4794 - val_loss: 1.3734 - val_acc: 0.3948\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.40696\n",
      "Epoch 67/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2185 - acc: 0.4807 - val_loss: 1.3729 - val_acc: 0.3930\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.40696\n",
      "Epoch 68/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2241 - acc: 0.4828 - val_loss: 1.3718 - val_acc: 0.3983\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.40696\n",
      "Epoch 69/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2160 - acc: 0.4737 - val_loss: 1.3771 - val_acc: 0.3913\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.40696\n",
      "Epoch 70/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2127 - acc: 0.4785 - val_loss: 1.3787 - val_acc: 0.3930\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.40696\n",
      "Epoch 71/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2089 - acc: 0.4824 - val_loss: 1.3776 - val_acc: 0.3878\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.40696\n",
      "Epoch 72/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2295 - acc: 0.4763 - val_loss: 1.3773 - val_acc: 0.3861\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.40696\n",
      "Epoch 73/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2132 - acc: 0.4785 - val_loss: 1.3786 - val_acc: 0.3878\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.40696\n",
      "Epoch 74/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2104 - acc: 0.4889 - val_loss: 1.3780 - val_acc: 0.3983\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.40696\n",
      "Epoch 75/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2060 - acc: 0.4828 - val_loss: 1.3763 - val_acc: 0.3878\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.40696\n",
      "Epoch 76/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2100 - acc: 0.4815 - val_loss: 1.3754 - val_acc: 0.3878\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.40696\n",
      "Epoch 77/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2127 - acc: 0.4794 - val_loss: 1.3782 - val_acc: 0.3896\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.40696\n",
      "Epoch 78/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2123 - acc: 0.4824 - val_loss: 1.3738 - val_acc: 0.3913\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.40696\n",
      "Epoch 79/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2059 - acc: 0.4711 - val_loss: 1.3742 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.40696\n",
      "Epoch 80/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2200 - acc: 0.4794 - val_loss: 1.3781 - val_acc: 0.3896\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.40696\n",
      "Epoch 81/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2084 - acc: 0.4837 - val_loss: 1.3829 - val_acc: 0.3896\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.40696\n",
      "Epoch 82/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2006 - acc: 0.4902 - val_loss: 1.3764 - val_acc: 0.3965\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.40696\n",
      "Epoch 83/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2059 - acc: 0.4924 - val_loss: 1.3798 - val_acc: 0.3983\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.40696\n",
      "Epoch 84/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2073 - acc: 0.4854 - val_loss: 1.3843 - val_acc: 0.3896\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.40696\n",
      "Epoch 85/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2067 - acc: 0.4902 - val_loss: 1.3808 - val_acc: 0.3896\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.40696\n",
      "Epoch 86/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2011 - acc: 0.4972 - val_loss: 1.3816 - val_acc: 0.3948\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.40696\n",
      "Epoch 87/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1983 - acc: 0.4972 - val_loss: 1.3786 - val_acc: 0.3913\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.40696\n",
      "Epoch 88/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1954 - acc: 0.4946 - val_loss: 1.3768 - val_acc: 0.3861\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.40696\n",
      "Epoch 89/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1954 - acc: 0.4907 - val_loss: 1.3812 - val_acc: 0.3930\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.40696\n",
      "Epoch 90/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1978 - acc: 0.4911 - val_loss: 1.3776 - val_acc: 0.3913\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.40696\n",
      "Epoch 91/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1926 - acc: 0.4946 - val_loss: 1.3868 - val_acc: 0.3826\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.40696\n",
      "Epoch 92/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.2087 - acc: 0.4889 - val_loss: 1.3770 - val_acc: 0.3913\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.40696\n",
      "Epoch 93/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1983 - acc: 0.4950 - val_loss: 1.3836 - val_acc: 0.3930\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.40696\n",
      "Epoch 94/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1981 - acc: 0.4854 - val_loss: 1.3806 - val_acc: 0.3965\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.40696\n",
      "Epoch 95/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1884 - acc: 0.5033 - val_loss: 1.3804 - val_acc: 0.3948\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.40696\n",
      "Epoch 96/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1923 - acc: 0.4963 - val_loss: 1.3830 - val_acc: 0.3913\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.40696\n",
      "Epoch 97/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1984 - acc: 0.4894 - val_loss: 1.3861 - val_acc: 0.3843\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.40696\n",
      "Epoch 98/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1957 - acc: 0.4915 - val_loss: 1.3794 - val_acc: 0.3861\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.40696\n",
      "Epoch 99/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1900 - acc: 0.5007 - val_loss: 1.3815 - val_acc: 0.3843\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.40696\n",
      "Epoch 100/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.1876 - acc: 0.4933 - val_loss: 1.3869 - val_acc: 0.3826\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.40696\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 100\n",
    "BATCH_SIZE = 64\n",
    "VERBOSE = 1\n",
    "\n",
    "#tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "#esCallBack = EarlyStopping(monitor='val_acc', min_delta=0, patience=12, verbose=0, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.01,patience=5, min_lr=0.0001)\n",
    "best_checkpoint = ModelCheckpoint('LSTM-ATTENTION-BF-energie.h5', monitor='val_acc', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "\t\tbatch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "\t\tverbose=1, # 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.\n",
    "\t\tvalidation_data=(X_test,y_test),\n",
    "\t\t#validation_split=VALIDATION_SPLIT,\n",
    "\t\tcallbacks=[reduce_lr,best_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_zaAetbPn5r"
   },
   "outputs": [],
   "source": [
    "import json,codecs\n",
    "import numpy as np\n",
    "def saveHist(path,history):\n",
    "\n",
    "    new_hist = {}\n",
    "    for key in list(history.history.keys()):\n",
    "        if type(history.history[key]) == np.ndarray:\n",
    "            new_hist[key] == history.history[key].tolist()\n",
    "        elif type(history.history[key]) == list:\n",
    "           if  type(history.history[key][0]) == np.float64:\n",
    "               new_hist[key] = list(map(float, history.history[key]))\n",
    "\n",
    "    #print(new_hist)\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_hist, f, separators=(',', ':'), sort_keys=True, indent=4) \n",
    "\n",
    "def loadHist(path):\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as f:\n",
    "        n = json.loads(f.read())\n",
    "    return n\n",
    "\n",
    "\n",
    "saveHist('LSTM-ATTENTION-BF-energie.hist',history)\n",
    "hist=loadHist('LSTM-ATTENTION-BF-energie.hist')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "m4GwqDEvUNH_",
    "outputId": "a4474133-947c-4259-f893-9739be25a881"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDwrOOedSeVBwzjmXyoOCc865VP8P/4cRECJCcZ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "acc=np.array(hist['acc'])\n",
    "#acc=acc[0:100]\n",
    "#acc=np.append(acc,[0.7])\n",
    "plt.plot(acc,'b--')\n",
    "plt.plot(hist['val_acc'],'b',label='DNNC')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "loss=np.array(hist['loss'])\n",
    "#loss=np.append(loss,[2])\n",
    "plt.plot(loss,'b--',label='train')\n",
    "plt.plot(hist['val_loss'],'b',label='test')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sw1Kh9q_Icyb"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 100 and 72 for 'dense_1_2/add' (op: 'Add') with input shapes: [?,100,72], [1,72,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1588\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 100 and 72 for 'dense_1_2/add' (op: 'Add') with input shapes: [?,100,72], [1,72,1].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4c3675e1c8ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM-ATTENTION-BF-energie.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    333\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 145\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mnode_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                         \u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(x, bias, data_format)\u001b[0m\n\u001b[1;32m   4018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 297\u001b[0;31m         \"Add\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3412\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3413\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3414\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1754\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1755\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1756\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1757\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 100 and 72 for 'dense_1_2/add' (op: 'Add') with input shapes: [?,100,72], [1,72,1]."
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('LSTM-ATTENTION-BF-energie.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9b79cd6acb42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Confusion Matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "print('Confusion Matrix')\n",
    "cm=confusion_matrix(np.argmax(y_test,axis=1), np.argmax(model.predict(X_test),axis=1))\n",
    "print(cm)\n",
    "scores = model.evaluate(X_test,y_test, verbose=0)\n",
    "print(\"Dados Validao %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "predicted = np.argmax(pred, axis=1)\n",
    "report = classification_report(np.argmax(y_test, axis=1), predicted,digits=4)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM-ATTENTION-BF-enegie.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
