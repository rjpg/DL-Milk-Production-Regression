{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CVFEr0r9M2MS",
    "outputId": "54c30c03-ad2b-48db-cf10-f06ea7ef9e01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjpg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rjpg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rjpg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rjpg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rjpg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rjpg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rjpg/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D \n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.layers.core import Activation, Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.layers import Input, Permute,  Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers import merge, Multiply\n",
    "\n",
    "K.set_image_dim_ordering(\"th\")\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "\n",
    "\n",
    "def attention_3d_block(inputs,timesteps):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    time_steps=timesteps\n",
    "    print(input_dim)\n",
    "    print(time_steps)\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, time_steps))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a) #, name='dim_reduction'\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1))(a) #, name='attention_vec ' % (depth)\n",
    "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    output_attention_mul= Multiply()([inputs, a_probs]) #name='attention_mul'\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "class LSTMATTBFnet:\n",
    "    @staticmethod\n",
    "    def build(timeSteps,variables,classes):\n",
    "        inputNet = Input(shape=(timeSteps,variables)) #batch_shape=(20, 7, 5) \n",
    "        #lstm=Bidirectional(LSTM(100,recurrent_dropout=0.4,dropout=0.4),merge_mode='concat')(inputNet) #worse using stateful=True\n",
    "        attention_mul = attention_3d_block(inputNet,timeSteps)\n",
    "        lstm=LSTM(100,recurrent_dropout=0.1,dropout=0.1,return_sequences=False)(attention_mul) #worse using stateful=True \n",
    "        \n",
    "        \n",
    "        # a softmax classifier\n",
    "        classificationLayer=Dense(classes,activation='softmax')(lstm)\n",
    "        \n",
    "        model=Model(inputNet,classificationLayer)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "bV85vySlM2MV",
    "outputId": "2ed26b33-b466-4fd6-d2aa-2ec77f64414d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# total shapes #############\n",
      "(2876, 72, 8)\n",
      "2876\n",
      "############# train shapes #############\n",
      "(2301, 72, 8)\n",
      "(2301, 1)\n",
      "############# test shapes #############\n",
      "(575, 72, 8)\n",
      "(575, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjpg/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def non_shuffling_train_test_split(X, y, test_size=0.2):\n",
    "    i = int((1 - test_size) * X.shape[0]) + 1\n",
    "    X_train, X_test = np.split(X, [i])\n",
    "    y_train, y_test = np.split(y, [i])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# load all data\n",
    "dfin = pd.read_csv('windows-size72-step12.csv',header=None)  #inputs 72 lines per sample\n",
    "dfout = pd.read_csv('out-class-size72-step12.csv',header=None)  #output classes \n",
    "\n",
    "total_inputs,total_output = dfin.as_matrix().astype(np.float32),dfout.as_matrix().astype(np.int32)\n",
    "\n",
    "# normalize\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "total_inputs = scaler.fit_transform(total_inputs)\n",
    "\n",
    "# every 72 lines is one input sample\n",
    "total_inputs = np.reshape(total_inputs, (-1,72,8))\n",
    "print(\"############# total shapes #############\")\n",
    "print(total_inputs.shape)\n",
    "print(total_output.size)\n",
    "\n",
    "train_inputs, test_inputs, train_output, test_output = non_shuffling_train_test_split(total_inputs, total_output, test_size=0.2)\n",
    "print(\"############# train shapes #############\")\n",
    "print(train_inputs.shape)\n",
    "print(train_output.shape)\n",
    "\n",
    "print(\"############# test shapes #############\")\n",
    "print(test_inputs.shape)\n",
    "print(test_output.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 775
    },
    "colab_type": "code",
    "id": "QH6FD43fM2Mc",
    "outputId": "fc143ce7-8198-4599-e698-7b4be8da88ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(575, 5)\n",
      "8\n",
      "72\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 8 and 72 for 'dense_1/add' (op: 'Add') with input shapes: [?,8,72], [1,72,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1588\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 8 and 72 for 'dense_1/add' (op: 'Add') with input shapes: [?,8,72], [1,72,1].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d0248a6fac7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mOPTIMIZER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMATTBFnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeSteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_ROWS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_COLS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNB_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n\u001b[1;32m     15\u001b[0m \tmetrics=[\"accuracy\"])\n",
      "\u001b[0;32m<ipython-input-5-d303f525f28b>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(timeSteps, variables, classes)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0minputNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeSteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#batch_shape=(20, 7, 5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#lstm=Bidirectional(LSTM(100,recurrent_dropout=0.4,dropout=0.4),merge_mode='concat')(inputNet) #worse using stateful=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mattention_mul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_3d_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputNet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimeSteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mlstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mul\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#worse using stateful=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d303f525f28b>\u001b[0m in \u001b[0;36mattention_3d_block\u001b[0;34m(inputs, timesteps)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#a = Reshape((input_dim, time_steps))(a) # this line is not useful. It's just to know which dimension is what.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mSINGLE_ATTENTION_VECTOR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, name='dim_reduction'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(x, bias, data_format)\u001b[0m\n\u001b[1;32m   4018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_first'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4020\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4021\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4022\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 297\u001b[0;31m         \"Add\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3412\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3413\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3414\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1754\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1755\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1756\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1757\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 8 and 72 for 'dense_1/add' (op: 'Add') with input shapes: [?,8,72], [1,72,1]."
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_ROWS, IMG_COLS = 72, 8 # input image dimensions\n",
    "NB_CLASSES = 5  # number of outputs = number of classes\n",
    "\n",
    "X_train = train_inputs\n",
    "y_train = np_utils.to_categorical(train_output, NB_CLASSES)\n",
    "X_test = test_inputs\n",
    "y_test = np_utils.to_categorical(test_output, NB_CLASSES)\n",
    "print(y_test.shape)\n",
    "\n",
    "OPTIMIZER = Adam()\n",
    "\n",
    "model = LSTMATTBFnet.build(timeSteps=IMG_ROWS,variables=IMG_COLS,classes=NB_CLASSES)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "#plot_model(modeltest, to_file=\"model.png\",show_shapes=True)\n",
    "from IPython.display import SVG,display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "modelSVG=SVG(model_to_dot(model,show_shapes=True).create(prog='dot', format='svg'))\n",
    "display(model)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 757
    },
    "colab_type": "code",
    "id": "1NNjAwLJNtMC",
    "outputId": "62958a86-19e5-40e1-8ea6-a6c5f7a0dfa5"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"553pt\" viewBox=\"0.00 0.00 428.50 553.00\" width=\"429pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 549)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-549 424.5,-549 424.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140209056487744 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140209056487744</title>\n",
       "<polygon fill=\"none\" points=\"85,-498.5 85,-544.5 371,-544.5 371,-498.5 85,-498.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.5\" y=\"-517.8\">input_1: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"218,-498.5 218,-544.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"218,-521.5 276,-521.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"276,-498.5 276,-544.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.5\" y=\"-529.3\">(None, 72, 8)</text>\n",
       "<polyline fill=\"none\" points=\"276,-521.5 371,-521.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323.5\" y=\"-506.3\">(None, 72, 8)</text>\n",
       "</g>\n",
       "<!-- 140208603905496 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140208603905496</title>\n",
       "<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 286,-461.5 286,-415.5 0,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66.5\" y=\"-434.8\">permute_1: Permute</text>\n",
       "<polyline fill=\"none\" points=\"133,-415.5 133,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"133,-438.5 191,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"191,-415.5 191,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.5\" y=\"-446.3\">(None, 72, 8)</text>\n",
       "<polyline fill=\"none\" points=\"191,-438.5 286,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.5\" y=\"-423.3\">(None, 8, 72)</text>\n",
       "</g>\n",
       "<!-- 140209056487744&#45;&gt;140208603905496 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140209056487744-&gt;140208603905496</title>\n",
       "<path d=\"M204.3228,-498.3799C195.0063,-489.2827 184.1777,-478.7088 174.2354,-469.0005\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"176.4451,-466.2663 166.8451,-461.784 171.5546,-471.2746 176.4451,-466.2663\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140208119942728 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140208119942728</title>\n",
       "<polygon fill=\"none\" points=\"35.5,-166.5 35.5,-212.5 420.5,-212.5 420.5,-166.5 35.5,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-185.8\">multiply_1: Multiply</text>\n",
       "<polyline fill=\"none\" points=\"173.5,-166.5 173.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"173.5,-189.5 231.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"231.5,-166.5 231.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"326\" y=\"-197.3\">[(None, 72, 8), (None, 72, 8)]</text>\n",
       "<polyline fill=\"none\" points=\"231.5,-189.5 420.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"326\" y=\"-174.3\">(None, 72, 8)</text>\n",
       "</g>\n",
       "<!-- 140209056487744&#45;&gt;140208119942728 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140209056487744-&gt;140208119942728</title>\n",
       "<path d=\"M264.8599,-498.2383C276.7728,-488.6164 288.5601,-476.3318 295,-462 333.8008,-375.6503 333.8008,-335.3497 295,-249 289.9688,-237.8033 281.6739,-227.8561 272.6051,-219.4519\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"274.7155,-216.6499 264.8599,-212.7617 270.1397,-221.9473 274.7155,-216.6499\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140208241615032 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140208241615032</title>\n",
       "<polygon fill=\"none\" points=\"13,-332.5 13,-378.5 273,-378.5 273,-332.5 13,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66.5\" y=\"-351.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"120,-332.5 120,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"120,-355.5 178,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"149\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"178,-332.5 178,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-363.3\">(None, 8, 72)</text>\n",
       "<polyline fill=\"none\" points=\"178,-355.5 273,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-340.3\">(None, 8, 72)</text>\n",
       "</g>\n",
       "<!-- 140208603905496&#45;&gt;140208241615032 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140208603905496-&gt;140208241615032</title>\n",
       "<path d=\"M143,-415.3799C143,-407.1745 143,-397.7679 143,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.5001,-388.784 143,-378.784 139.5001,-388.784 146.5001,-388.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140208119941608 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140208119941608</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 286,-295.5 286,-249.5 0,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"66.5\" y=\"-268.8\">permute_2: Permute</text>\n",
       "<polyline fill=\"none\" points=\"133,-249.5 133,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"133,-272.5 191,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"191,-249.5 191,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.5\" y=\"-280.3\">(None, 8, 72)</text>\n",
       "<polyline fill=\"none\" points=\"191,-272.5 286,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.5\" y=\"-257.3\">(None, 72, 8)</text>\n",
       "</g>\n",
       "<!-- 140208241615032&#45;&gt;140208119941608 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140208241615032-&gt;140208119941608</title>\n",
       "<path d=\"M143,-332.3799C143,-324.1745 143,-314.7679 143,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"146.5001,-305.784 143,-295.784 139.5001,-305.784 146.5001,-305.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140208119941608&#45;&gt;140208119942728 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140208119941608-&gt;140208119942728</title>\n",
       "<path d=\"M166.6772,-249.3799C175.9937,-240.2827 186.8223,-229.7088 196.7646,-220.0005\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"199.4454,-222.2746 204.1549,-212.784 194.5549,-217.2663 199.4454,-222.2746\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140208119942112 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140208119942112</title>\n",
       "<polygon fill=\"none\" points=\"100.5,-83.5 100.5,-129.5 355.5,-129.5 355.5,-83.5 100.5,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.5\" y=\"-102.8\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"202.5,-83.5 202.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"202.5,-106.5 260.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"260.5,-83.5 260.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"308\" y=\"-114.3\">(None, 72, 8)</text>\n",
       "<polyline fill=\"none\" points=\"260.5,-106.5 355.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"308\" y=\"-91.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140208119942728&#45;&gt;140208119942112 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140208119942728-&gt;140208119942112</title>\n",
       "<path d=\"M228,-166.3799C228,-158.1745 228,-148.7679 228,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"231.5001,-139.784 228,-129.784 224.5001,-139.784 231.5001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140208103290808 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140208103290808</title>\n",
       "<polygon fill=\"none\" points=\"102,-.5 102,-46.5 354,-46.5 354,-.5 102,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-19.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"209,-.5 209,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"209,-23.5 267,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"267,-.5 267,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310.5\" y=\"-31.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"267,-23.5 354,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"310.5\" y=\"-8.3\">(None, 5)</text>\n",
       "</g>\n",
       "<!-- 140208119942112&#45;&gt;140208103290808 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140208119942112-&gt;140208103290808</title>\n",
       "<path d=\"M228,-83.3799C228,-75.1745 228,-65.7679 228,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"231.5001,-56.784 228,-46.784 224.5001,-56.784 231.5001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import SVG,display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "modelSVG=SVG(model_to_dot(model,show_shapes=True).create(prog='dot', format='svg'))\n",
    "display(modelSVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7309
    },
    "colab_type": "code",
    "id": "zpWno9_7PNh-",
    "outputId": "412787ea-c55e-4bf0-ea21-e4808bb777d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0615 16:31:50.716738 140209092077440 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2301 samples, validate on 575 samples\n",
      "Epoch 1/100\n",
      "2301/2301 [==============================] - 5s 2ms/step - loss: 1.6092 - acc: 0.2177 - val_loss: 1.6187 - val_acc: 0.1217\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.12174, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 2/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.6071 - acc: 0.2208 - val_loss: 1.6159 - val_acc: 0.1217\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.12174\n",
      "Epoch 3/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.5814 - acc: 0.2755 - val_loss: 1.5860 - val_acc: 0.2435\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.12174 to 0.24348, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 4/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.5225 - acc: 0.3229 - val_loss: 1.5625 - val_acc: 0.2470\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.24348 to 0.24696, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 5/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.4967 - acc: 0.3351 - val_loss: 1.5263 - val_acc: 0.2591\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.24696 to 0.25913, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 6/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.4731 - acc: 0.3385 - val_loss: 1.5127 - val_acc: 0.2713\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.25913 to 0.27130, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 7/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.4481 - acc: 0.3555 - val_loss: 1.5167 - val_acc: 0.3009\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.27130 to 0.30087, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 8/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.4472 - acc: 0.3629 - val_loss: 1.4625 - val_acc: 0.3148\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.30087 to 0.31478, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 9/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.4271 - acc: 0.3677 - val_loss: 1.4597 - val_acc: 0.3113\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.31478\n",
      "Epoch 10/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.4117 - acc: 0.3694 - val_loss: 1.4445 - val_acc: 0.3113\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.31478\n",
      "Epoch 11/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.4059 - acc: 0.3824 - val_loss: 1.4272 - val_acc: 0.3270\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.31478 to 0.32696, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 12/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.4019 - acc: 0.3720 - val_loss: 1.4307 - val_acc: 0.3339\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.32696 to 0.33391, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 13/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.4037 - acc: 0.3820 - val_loss: 1.4366 - val_acc: 0.3252\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.33391\n",
      "Epoch 14/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.4012 - acc: 0.3907 - val_loss: 1.4300 - val_acc: 0.3426\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.33391 to 0.34261, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 15/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3947 - acc: 0.3868 - val_loss: 1.4296 - val_acc: 0.3687\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.34261 to 0.36870, saving model to LSTM-ATTENTION-AF-energie.h5\n",
      "Epoch 16/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3965 - acc: 0.3824 - val_loss: 1.4607 - val_acc: 0.3113\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.36870\n",
      "Epoch 17/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3866 - acc: 0.3850 - val_loss: 1.4228 - val_acc: 0.3600\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.36870\n",
      "Epoch 18/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3782 - acc: 0.3855 - val_loss: 1.4215 - val_acc: 0.3496\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.36870\n",
      "Epoch 19/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3770 - acc: 0.3894 - val_loss: 1.4183 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.36870\n",
      "Epoch 20/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3745 - acc: 0.3968 - val_loss: 1.4173 - val_acc: 0.3600\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.36870\n",
      "Epoch 21/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3706 - acc: 0.3881 - val_loss: 1.4166 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.36870\n",
      "Epoch 22/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3843 - acc: 0.3916 - val_loss: 1.4136 - val_acc: 0.3496\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.36870\n",
      "Epoch 23/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3730 - acc: 0.3911 - val_loss: 1.4138 - val_acc: 0.3635\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.36870\n",
      "Epoch 24/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3725 - acc: 0.3937 - val_loss: 1.4125 - val_acc: 0.3617\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.36870\n",
      "Epoch 25/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3637 - acc: 0.3864 - val_loss: 1.4119 - val_acc: 0.3461\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.36870\n",
      "Epoch 26/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3707 - acc: 0.3946 - val_loss: 1.4114 - val_acc: 0.3583\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.36870\n",
      "Epoch 27/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3622 - acc: 0.3942 - val_loss: 1.4097 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.36870\n",
      "Epoch 28/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3662 - acc: 0.3959 - val_loss: 1.4099 - val_acc: 0.3617\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.36870\n",
      "Epoch 29/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3743 - acc: 0.4003 - val_loss: 1.4100 - val_acc: 0.3513\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.36870\n",
      "Epoch 30/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3697 - acc: 0.3963 - val_loss: 1.4081 - val_acc: 0.3513\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.36870\n",
      "Epoch 31/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3647 - acc: 0.3942 - val_loss: 1.4068 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.36870\n",
      "Epoch 32/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3625 - acc: 0.4033 - val_loss: 1.4076 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.36870\n",
      "Epoch 33/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3581 - acc: 0.3950 - val_loss: 1.4080 - val_acc: 0.3600\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.36870\n",
      "Epoch 34/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3729 - acc: 0.3864 - val_loss: 1.4089 - val_acc: 0.3565\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.36870\n",
      "Epoch 35/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3682 - acc: 0.3985 - val_loss: 1.4083 - val_acc: 0.3478\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.36870\n",
      "Epoch 36/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3677 - acc: 0.4024 - val_loss: 1.4077 - val_acc: 0.3374\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.36870\n",
      "Epoch 37/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3664 - acc: 0.3955 - val_loss: 1.4059 - val_acc: 0.3513\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.36870\n",
      "Epoch 38/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3774 - acc: 0.3850 - val_loss: 1.4100 - val_acc: 0.3496\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.36870\n",
      "Epoch 39/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3674 - acc: 0.3977 - val_loss: 1.4112 - val_acc: 0.3374\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.36870\n",
      "Epoch 40/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3551 - acc: 0.3990 - val_loss: 1.4098 - val_acc: 0.3357\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.36870\n",
      "Epoch 41/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3623 - acc: 0.3998 - val_loss: 1.4059 - val_acc: 0.3426\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.36870\n",
      "Epoch 42/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3648 - acc: 0.4016 - val_loss: 1.4062 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.36870\n",
      "Epoch 43/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3588 - acc: 0.3985 - val_loss: 1.4065 - val_acc: 0.3565\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.36870\n",
      "Epoch 44/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3614 - acc: 0.4020 - val_loss: 1.4062 - val_acc: 0.3322\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.36870\n",
      "Epoch 45/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3545 - acc: 0.4063 - val_loss: 1.4058 - val_acc: 0.3478\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.36870\n",
      "Epoch 46/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3595 - acc: 0.3972 - val_loss: 1.4041 - val_acc: 0.3478\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.36870\n",
      "Epoch 47/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3579 - acc: 0.4103 - val_loss: 1.4069 - val_acc: 0.3391\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.36870\n",
      "Epoch 48/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3663 - acc: 0.4016 - val_loss: 1.4080 - val_acc: 0.3270\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.36870\n",
      "Epoch 49/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3528 - acc: 0.3994 - val_loss: 1.4052 - val_acc: 0.3530\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.36870\n",
      "Epoch 50/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3653 - acc: 0.3877 - val_loss: 1.4055 - val_acc: 0.3304\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.36870\n",
      "Epoch 51/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3606 - acc: 0.3994 - val_loss: 1.4051 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.36870\n",
      "Epoch 52/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3581 - acc: 0.4029 - val_loss: 1.4041 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.36870\n",
      "Epoch 53/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3548 - acc: 0.3972 - val_loss: 1.4025 - val_acc: 0.3461\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.36870\n",
      "Epoch 54/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3587 - acc: 0.4003 - val_loss: 1.4049 - val_acc: 0.3304\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.36870\n",
      "Epoch 55/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3612 - acc: 0.3916 - val_loss: 1.4024 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.36870\n",
      "Epoch 56/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3539 - acc: 0.4033 - val_loss: 1.4074 - val_acc: 0.3374\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.36870\n",
      "Epoch 57/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3574 - acc: 0.3994 - val_loss: 1.4040 - val_acc: 0.3513\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.36870\n",
      "Epoch 58/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3550 - acc: 0.4016 - val_loss: 1.4044 - val_acc: 0.3478\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.36870\n",
      "Epoch 59/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3581 - acc: 0.4076 - val_loss: 1.4049 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.36870\n",
      "Epoch 60/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3478 - acc: 0.4007 - val_loss: 1.4031 - val_acc: 0.3322\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.36870\n",
      "Epoch 61/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3589 - acc: 0.3998 - val_loss: 1.4028 - val_acc: 0.3374\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.36870\n",
      "Epoch 62/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3551 - acc: 0.3998 - val_loss: 1.4024 - val_acc: 0.3391\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.36870\n",
      "Epoch 63/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3560 - acc: 0.4076 - val_loss: 1.4051 - val_acc: 0.3339\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.36870\n",
      "Epoch 64/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3568 - acc: 0.3994 - val_loss: 1.4035 - val_acc: 0.3478\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.36870\n",
      "Epoch 65/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3507 - acc: 0.4098 - val_loss: 1.4044 - val_acc: 0.3409\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.36870\n",
      "Epoch 66/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3586 - acc: 0.4042 - val_loss: 1.4049 - val_acc: 0.3496\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.36870\n",
      "Epoch 67/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3550 - acc: 0.4016 - val_loss: 1.4053 - val_acc: 0.3496\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.36870\n",
      "Epoch 68/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3541 - acc: 0.3937 - val_loss: 1.4053 - val_acc: 0.3391\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.36870\n",
      "Epoch 69/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3624 - acc: 0.3916 - val_loss: 1.4030 - val_acc: 0.3565\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.36870\n",
      "Epoch 70/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3523 - acc: 0.4094 - val_loss: 1.4035 - val_acc: 0.3374\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.36870\n",
      "Epoch 71/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3530 - acc: 0.4003 - val_loss: 1.4043 - val_acc: 0.3322\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.36870\n",
      "Epoch 72/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3473 - acc: 0.4159 - val_loss: 1.4016 - val_acc: 0.3530\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.36870\n",
      "Epoch 73/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3455 - acc: 0.4059 - val_loss: 1.4021 - val_acc: 0.3530\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.36870\n",
      "Epoch 74/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3510 - acc: 0.4016 - val_loss: 1.4012 - val_acc: 0.3496\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.36870\n",
      "Epoch 75/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3489 - acc: 0.4007 - val_loss: 1.4018 - val_acc: 0.3391\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.36870\n",
      "Epoch 76/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3476 - acc: 0.4163 - val_loss: 1.4013 - val_acc: 0.3426\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.36870\n",
      "Epoch 77/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3493 - acc: 0.4094 - val_loss: 1.4037 - val_acc: 0.3478\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.36870\n",
      "Epoch 78/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3488 - acc: 0.4072 - val_loss: 1.4045 - val_acc: 0.3426\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.36870\n",
      "Epoch 79/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3515 - acc: 0.4029 - val_loss: 1.4008 - val_acc: 0.3496\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.36870\n",
      "Epoch 80/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3518 - acc: 0.4059 - val_loss: 1.4037 - val_acc: 0.3409\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.36870\n",
      "Epoch 81/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3485 - acc: 0.4050 - val_loss: 1.3993 - val_acc: 0.3409\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.36870\n",
      "Epoch 82/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3484 - acc: 0.4098 - val_loss: 1.4043 - val_acc: 0.3565\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.36870\n",
      "Epoch 83/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3424 - acc: 0.4116 - val_loss: 1.3991 - val_acc: 0.3426\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.36870\n",
      "Epoch 84/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3441 - acc: 0.4020 - val_loss: 1.3980 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.36870\n",
      "Epoch 85/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3447 - acc: 0.4068 - val_loss: 1.3996 - val_acc: 0.3496\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.36870\n",
      "Epoch 86/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3418 - acc: 0.4124 - val_loss: 1.4004 - val_acc: 0.3600\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.36870\n",
      "Epoch 87/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3592 - acc: 0.4029 - val_loss: 1.4032 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.36870\n",
      "Epoch 88/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3405 - acc: 0.4103 - val_loss: 1.3991 - val_acc: 0.3513\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.36870\n",
      "Epoch 89/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3416 - acc: 0.4016 - val_loss: 1.4000 - val_acc: 0.3391\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.36870\n",
      "Epoch 90/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3436 - acc: 0.4081 - val_loss: 1.3951 - val_acc: 0.3530\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.36870\n",
      "Epoch 91/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3480 - acc: 0.4011 - val_loss: 1.3998 - val_acc: 0.3339\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.36870\n",
      "Epoch 92/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3383 - acc: 0.4146 - val_loss: 1.3980 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.36870\n",
      "Epoch 93/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3404 - acc: 0.4081 - val_loss: 1.3985 - val_acc: 0.3635\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.36870\n",
      "Epoch 94/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3403 - acc: 0.4137 - val_loss: 1.3976 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.36870\n",
      "Epoch 95/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3505 - acc: 0.4094 - val_loss: 1.3987 - val_acc: 0.3443\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.36870\n",
      "Epoch 96/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3485 - acc: 0.4124 - val_loss: 1.3990 - val_acc: 0.3426\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.36870\n",
      "Epoch 97/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3351 - acc: 0.4163 - val_loss: 1.3971 - val_acc: 0.3513\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.36870\n",
      "Epoch 98/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3456 - acc: 0.4063 - val_loss: 1.3971 - val_acc: 0.3548\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.36870\n",
      "Epoch 99/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3474 - acc: 0.4050 - val_loss: 1.3964 - val_acc: 0.3409\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.36870\n",
      "Epoch 100/100\n",
      "2301/2301 [==============================] - 4s 2ms/step - loss: 1.3363 - acc: 0.4050 - val_loss: 1.3979 - val_acc: 0.3565\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.36870\n"
     ]
    }
   ],
   "source": [
    "NB_EPOCH = 100\n",
    "BATCH_SIZE = 64\n",
    "VERBOSE = 1\n",
    "\n",
    "#tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "#esCallBack = EarlyStopping(monitor='val_acc', min_delta=0, patience=12, verbose=0, mode='max')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.01,patience=5, min_lr=0.0001)\n",
    "best_checkpoint = ModelCheckpoint('LSTM-ATTENTION-AF-energie.h5', monitor='val_acc', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "\t\tbatch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "\t\tverbose=1, # 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.\n",
    "\t\tvalidation_data=(X_test,y_test),\n",
    "\t\t#validation_split=VALIDATION_SPLIT,\n",
    "\t\tcallbacks=[reduce_lr,best_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_zaAetbPn5r"
   },
   "outputs": [],
   "source": [
    "import json,codecs\n",
    "import numpy as np\n",
    "def saveHist(path,history):\n",
    "\n",
    "    new_hist = {}\n",
    "    for key in list(history.history.keys()):\n",
    "        if type(history.history[key]) == np.ndarray:\n",
    "            new_hist[key] == history.history[key].tolist()\n",
    "        elif type(history.history[key]) == list:\n",
    "           if  type(history.history[key][0]) == np.float64:\n",
    "               new_hist[key] = list(map(float, history.history[key]))\n",
    "\n",
    "    #print(new_hist)\n",
    "    with codecs.open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_hist, f, separators=(',', ':'), sort_keys=True, indent=4) \n",
    "\n",
    "def loadHist(path):\n",
    "    with codecs.open(path, 'r', encoding='utf-8') as f:\n",
    "        n = json.loads(f.read())\n",
    "    return n\n",
    "\n",
    "\n",
    "saveHist('LSTM-ATTENTION-AF-energie.hist',history)\n",
    "hist=loadHist('LSTM-ATTENTION-AF-energie.hist')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "m4GwqDEvUNH_",
    "outputId": "594f2a63-8ec6-4afd-a17e-ccae4fca934f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxUJRFEUJyP8D1ku5HfRNezAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niPw/CXc1uKwea0QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "acc=np.array(hist['acc'])\n",
    "#acc=acc[0:100]\n",
    "#acc=np.append(acc,[0.7])\n",
    "plt.plot(acc,'b--')\n",
    "plt.plot(hist['val_acc'],'b',label='DNNC')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "loss=np.array(hist['loss'])\n",
    "#loss=np.append(loss,[2])\n",
    "plt.plot(loss,'b--',label='train')\n",
    "plt.plot(hist['val_loss'],'b',label='test')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sw1Kh9q_Icyb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM-ATTENTION-AF-enegie.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
